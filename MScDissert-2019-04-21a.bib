@article{Abdullah2012,
abstract = {Y o u r Q u e s t i o n s A n s w e r e d The layman's perspective on technical theory and practical applications of mapping and GIS " {\ldots}the main difference between the conventional lidar technology and lidar operating in Gieger mode (photon counting) technology is in the forms and methods that the back scattered light (or refl ected signal) is received and processed. " Question: I heard about a new technology in lidar called Geiger mode, what is it? Anonymous Reader Dr. Abdullah: The principle behind the operation of current lidar systems is based on a pulsed laser beam that is sent in a specifi c direction until it hits an object in its path. The pulsed light then interacts with molecules of the object and scatters in various directions with a portion refl ected back (backscattered) towards its origin (the lidar system). The backscattered light sent in the direction of the sensor is collected by telescopes and conducted into a detection unit (detector or receiver). After passing through fi lters, the signal (represented by photons) is received by a single element (i.e., non-pixilated) detector that detects and processes multi-photon returns. The new technology that you refer to differs from conventional lidar systems by splitting each pulse into an array of sub-pulses, each of which is considered to be a pulse on its own. Therefore, the main difference between the conventional lidar technology and lidar operating in Gieger mode (photon counting) technology is in the forms and methods that the back scattered light (or refl ected signal) is received and processed. Conventional lidar systems use lasers with suffi cient energy per pulse, in combination with optics of suffi cient aperture, to work with return signals of thousands of photons per shot. The high signal level approach requires laser repetition rates of hundreds of thousands of pulses per second to obtain contiguous coverage of the terrain at sub meter spatial resolution. Such requirements impact the design parameters of these systems and limit scalability in terms of power and hardware. In the last decade, a new paradigm in lidar design has evolved, utilizing the Geiger mode photon counting detector technology. The concept of Geiger mode, or photon counting-based lidar, is centered on creating an array of lidar point clouds from a single emitted laser pulse. The foundation of Geiger mode lidar is very light-sensitive, solid-state photodetectors. These devices are able to detect a low intensity incident fl ux of light (down to the single photon) and provide precise information on the number of photons arriving at the detector and their arrival time. In these lidar sensors, a relatively low energy laser (few micro joules) transmits in the form of sub nanosecond pulses at a high rate. The transmitted pulse then illuminates a ground surface area with a size that is dependent on fl ying altitude and laser aperture. The backscat-ter from the illuminated ground area is focused by a diffraction-limited telescope, or Diffractive Optical Element (DOE), and then projected onto a segmented single-photon-threshold Geiger-mode detector (receiver). The size, in terms of pixels, of the segmented anode can be tailored for each instrument. Some designs for the photon counting system are based on locating the DOE after the laser leaves the transmitter, resulting in multiple sub-pulses hitting the target. Each of these back-scattered sub-pulses is then directed by a telescope onto a single-photon-threshold Geiger-mode detector within the segmented (array) of detectors (See Figure 1).},
annote = {From Duplicate 2 (Mapping Matters - Abdullah, Qassim)

description of panchromatic image s and CCD effects on resolution

basics of pan sharpening

defines a multiband raster dataset

defines RGB, IHS, etc, in layman's terms},
author = {Abdullah, Qassim},
doi = {10.14358/PERS.81.11.831},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2013 - Abdullah QA - Mapping Matters.pdf:pdf},
isbn = {978-1-4244-3985-0},
issn = {00991112},
journal = {Photogrammetric Engineering {\&} Remote Sensing},
keywords = {American Society for Photogrammetry and Remote Sen,Mapping Matters,March 2009,No.3,Photogrammetric Engineering and Remote Sensing,Vol. 75,pp. 231-232},
number = {July},
pages = {664--665},
title = {{Mapping Matters}},
year = {2012}
}
@article{Ansari2016,
abstract = {Multi-resolution analysis (MRA) has been successfully used in image processing with the recent emergence of applications to texture classification. Several studies have investigated the discriminating power of wavelet-based features in various applications such as image compression, image denoising, and classification of natural textures. Recently, the curvelet and contourlet transforms have emerged as new multi-resolution analysis tools to deal with non-linear singularities present in the image. This article explores and proposes a texture based classification of remotely sensed multispectral images using features derived from the wavelet, curvelet and contourlet transforms. These features characterize the textural properties of the images and are used to train the classifier to recognize each texture class. Using these MRA based feature descriptors class separability is defined in feature space. The results are compared with Grey Level Co-occurrence Matrix (GLCM) based statistical features.},
author = {Ansari, Rizwan Ahmed and Buddhiraju, Krishna Mohan},
doi = {10.1109/IGARSS.2016.7729711},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2016 - Ansari RA, Buddhiraju KM - Textural classification based on wavelet, curvelet and contourlet features.pdf:pdf},
isbn = {9781509033324},
journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
keywords = {Wavelets,contourlets,curvelets,gray level co-occurrence matrix,textural features},
pages = {2753--2756},
publisher = {IEEE},
title = {{Textural classification based on wavelet, curvelet and contourlet features}},
volume = {2016-Novem},
year = {2016}
}
@misc{arcmap_10-3,
abstract = {An overview of pansharpening fundamentals},
annote = {From Duplicate 2 (ArcGIS - Fundamentals of panchromatic sharpening - ArcGIS)

describes a bunch of ways pansharpening can be done

describes ihs transform
brovey
esri
simple means
gram-schmidt
etc.},
author = {ArcGIS},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/ArcGIS - Fundamentals of panchromatic sharpening.pdf:pdf},
keywords = {panchromatic,pansharpening,sharpening},
mendeley-tags = {panchromatic,pansharpening,sharpening},
publisher = {ArcGIS},
title = {{ArcGIS - Fundamentals of panchromatic sharpening}},
url = {http://desktop.arccgis.com/en/arcmap/10.3/manage-data/raster-and-images/fundamentals-of-panchromatic-sharpening.htm http://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/fundamentals-of-panchromatic-sharpening.htm},
year = {2018}
}
@article{Baraldi2017,
author = {Baraldi, Andrea and Despini, Francesca and Teggi, Sergio},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2017 - MS Image Panchromatic Sharpening.pdf:pdf},
pages = {1--39},
title = {{Multi-spectral Image Panchromatic Sharpening – Outcome and Process Quality Assessment Protocol}},
url = {https://arxiv.org/ftp/arxiv/papers/1701/1701.01942.pdf},
year = {2017}
}
@article{Buscema2015,
author = {Buscema, Massimo},
doi = {10.3109/10826089809115872},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/1998 - Buscema M - Recirculation Neural Networks, Substance Use {\&} Misuse.pdf:pdf},
number = {May},
title = {{Recirculation Neural Networks}},
year = {2015}
}
@article{Chakravortty2014,
annote = {From Duplicate 2 (Fusion of hyperspectral and multispectral image data for enhancement of spectral and spatial resolution - Chakravortty, Somdatta; Subramaniam, Pallavi)

fusion of hyperspectral and MSI

discusses band mapping issues},
author = {Chakravortty, Somdatta and Subramaniam, Pallavi},
doi = {10.5194/isprsarchives-XL-8-1099-2014},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2014 - Chakravortty S, Subramaniam P - Fusion of Hyperspectral and MS Image Data for Enhancement of Spectral and Spatial Resolution.pdf:pdf},
issn = {16821750},
journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
keywords = {Band remapping,Data fusion,Hyperspectral,Multispectral,Spectral re-sampling},
number = {8},
pages = {1099--1103},
title = {{Fusion of hyperspectral and multispectral image data for enhancement of spectral and spatial resolution}},
volume = {40},
year = {2014}
}
@misc{Delp2018,
author = {Delp, Edward J.},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/Purdue - SAR Image Processing.pdf:pdf},
institution = {Purdue},
keywords = {SAR},
mendeley-tags = {SAR},
title = {{Purdue - SAR Image Processing.pdf}},
year = {2018}
}
@article{Dong2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.00092v3},
author = {Dong, Chao and Loy, Chen Change and He, Kaiming},
doi = {10.1109/TPAMI.2015.2439281},
eprint = {arXiv:1501.00092v3},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2016 - Dong C, et al. - Image Super-Resolution Using Deep CNNs - IEEE[V38N2Feb2016].pdf:pdf;:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2015 - Dong C, et al. - Image Super-Resolution Using Deep CNNs - IEEE - arXiv[1501.00092].pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {295--307},
publisher = {IEEE},
title = {{Image Super-Resolution Using Deep Convolutional Networks}},
volume = {38},
year = {2016}
}
@article{Engineering2014,
annote = {From Duplicate 2 (A Review on Image Processing Techniques for Synthetic Aperture Radar ( SAR ) Images - Engineering, Communication)

ice type classification by pixel

Wavelet Coefficient Shrinkage (WCS)
SWT
adaptive median filtering},
author = {Engineering, Communication},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2014 - Geetha P, Chitradevi B - A review on image processing techniques forsynthetic aperture radar sar images.pdf:pdf},
keywords = {feature extraction and classification,preprocessing,sea ice,segmentation,synthetic aperture radar},
pages = {3823--3829},
title = {{A Review on Image Processing Techniques for Synthetic Aperture Radar ( SAR ) Images}},
year = {2014}
}
@article{Garcia-Pineda2008,
abstract = {Synthetic Aperture Radar (SAR) satellite images have proven to be a successful tool for identifying oil slicks. Natural oil seeps can be detected as elongated, radar-dark slicks in SAR images. Use of SAR images for seep detection is enhanced by a Texture Classifying Neural Network Algorithm (TCNNA), which delineates areas where layers of floating oil suppress Bragg scattering. The effect is strongly influenced by wind strength and sea state. A multi orientation Leung-Malik filter bank [1] is used to identify slick shapes under projection of edges. By integrating ancillary data consisting of the incidence angle, descriptors of texture and environmental variables, considerable accuracy were added to the classification ability to discriminate false targets from oil slicks and look-alike pixels. The reliability of the TCNNA is measured after processing 71 images containing oil slicks.},
annote = {From Duplicate 2 (Synthetic aperture radar image processing using the supervised textural-neural network classification algorithm. - Garcia-Pineda, Oscar; MacDonald, Ian; Zimmer, Beate)

frrd forward neura; network for regression problems in oil slicks etc.},
author = {Garcia-Pineda, Oscar and MacDonald, Ian and Zimmer, Beate},
doi = {10.1109/IGARSS.2008.4779960},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2008 - Garcia-Pineda O, Macdonald I, Zimmer B - SAR Image Processing using the SUpervised Textural NN Classification Algorithm.pdf:pdf},
isbn = {9781424428083},
journal = {International Geoscience and Remote Sensing Symposium (IGARSS)},
number = {1},
pages = {IV -- 1265--IV -- 1268},
publisher = {IEEE},
title = {{Synthetic aperture radar image processing using the supervised textural-neural network classification algorithm.}},
volume = {4},
year = {2008}
}
@article{Gharbia2014,
abstract = {***talk about tradition pixel base fusion method {\#}{\#}{\#}not relevent},
annote = {From Duplicate 1 (Image Fusion Techniques in Remote Sensing - Gharbia, R.; Azar, A.T.; Baz, A.E.; Hassanien, A.E.)

***talk about tradition pixel base fusion method {\#}{\#}{\#}not relevant},
archivePrefix = {arXiv},
arxivId = {1403.5473},
author = {Gharbia, R. and Azar, A.T. and Baz, A.E. and Hassanien, A.E.},
doi = {10.1210/jc.2007-2450},
eprint = {1403.5473},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2014 - Gharbia R, Azar AT, El Baz A, Hassanien AE - Image Fusion Techniques in Remote Sensing.pdf:pdf},
isbn = {978-0-12-369407-2},
journal = {arXiv preprint arXiv: {\ldots}},
keywords = {-- image fusion,and wavelet transform,at,brovey,different times,ihs,imaging sensors or from,in using,pan-sharpening,pca,the same imaging sensor,there are several benefits,transform},
title = {{Image Fusion Techniques in Remote Sensing}},
url = {http://arxiv.org/abs/1403.5473},
year = {2014}
}
@unpublished{Guida2018,
annote = {From Duplicate 2 (SAR , OPTICAL AND LIDAR DATA FUSION FOR THE HIGH RESOLUTION MAPPING OF NATURAL PROTECTED AREAS - Guida, Raffaella; Marcello, J; Eugenio, F)

SVM {\textgreater} Maximum Likelihood

SAr textural image + MS optical images},
author = {Guida, Raffaella and Marcello, J and Eugenio, F and Eudenio, F},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/Guida R, Marcello J, Eugenio F - SAR, Optical and LIDAR Data Fusion for the High Resolution Mapping of Natural Protected Areas.pdf:pdf},
institution = {Surrey Space Centre, University of Surrey , 0Instituto de Oceanograf{\'{i}}a y Cambio Global ( IOCAG ), Universidad de Las Palmas de Gran Canaria},
title = {{SAR , OPTICAL AND LIDAR DATA FUSION FOR THE HIGH RESOLUTION MAPPING OF NATURAL PROTECTED AREAS}},
volume = {1}
}
@article{Hore2010,
author = {Hor{\'{e}}, Alain},
doi = {10.1109/ICPR.2010.579},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2010 - Hore A, Ziou D - Image quality metrics PSNR vs SSIM - IEEE 05596999.pdf:pdf},
journal = {2010 20th International Conference on Pattern Recognition},
keywords = {-psnr,image quality metrics,ssim},
pages = {2366--2369},
publisher = {IEEE},
title = {{Image quality metrics : PSNR vs . SSIM}},
year = {2010}
}
@article{Howes2007,
author = {Howes, Michael and Wortley, Liana and Potts, Ruth and Dedekorkut-howes, Aysin and Serrao-neumann, Silvia and Davidson, Julie and Smith, Timothy and Nunn, Patrick},
doi = {10.3390/su9020165},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2017 - Environmental Sustainability.pdf:pdf},
keywords = {environment,environmental governance,environmental outcomes,environmental policy,sustainable development},
pages = {1--17},
title = {{Environmental Sustainability : A Case of Policy Implementation Failure ?}},
year = {2007}
}
@article{Iervolino,
author = {Iervolino, Pasquale},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Yahia Oualid - IGARSS Decision Level Data Fusion for Soil Moisture Content Estimation.pdf:pdf},
pages = {1--4},
title = {{WEIGHTS BASED DECISION LEVEL DATA FUSION OF LANDSAT-8 AND SENTINEL-1}}
}
@article{Iervolino2018,
annote = {From Duplicate 2 (Land Classification using a novel Multispectral and SAR data Fu- sion in Doha area 2 Fusion Algorithm 3 Case Study - Iervolino, Pasquale; Surrey, Surrey; Centre, Space; Kingdom, United)

uses a similar fusion method to Rea

accuracy up to 86.52{\%} with fused data},
author = {Iervolino, Pasquale and Surrey, Surrey and Centre, Space and Kingdom, United},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Iervolino P, Guida R, Ayesh-Meagher A - EUSAR - Land Classification using a novel MS and SAR data fusion in Doha area.pdf:pdf},
isbn = {9783800746361},
issn = {21974403},
pages = {512--516},
title = {{Land Classification using a novel Multispectral and SAR data Fu- sion in Doha area 2 Fusion Algorithm 3 Case Study}},
year = {2018}
}
@article{Isola2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.07004v3},
author = {Isola, Phillip and Efros, Alexei A and Ai, Berkeley and Berkeley, U C},
eprint = {arXiv:1611.07004v3},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Isola P, et al.pdf - Image-to-Image Translatino with Conditional Adversarial Networks.pdf:pdf},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
year = {2018}
}
@article{Kandaswamy2005,
abstract = {We address the problem of efficiency in texture analysis for synthetic aperture radar (SAR) imagery. Motivated by the statistical occupancy model, we introduce the notion of patch reoccurrences. Using the reoccurrences, we propose the use of approximate textural features in analysis of SAR images. We describe how the proposed approximate features can be extracted for two popular texture analysis methods{\&}8212;the gray-level cooccurrence matrix and Gabor wavelets. Results on image texture classification show that the proposed method can provide an improved efficiency in the analysis of SAR imagery, without introducing any significant degradation in the classification results.},
annote = {From Duplicate 2 (Efficient texture analysis of SAR imagery - Kandaswamy, Umasankar; Adjeroh, Donald A.; Lee, M. C.)

focuses on advanced methods of texturual feature extraction (Gabor wavelets and other wavelet based methods).

highlights texture as a critical component 

context is classification of land used

conclusions about supervised being more successful than unsupervised means.

wavelet transforms increase the speed and efficiency of classification as opposed to necessarily the accuracy},
author = {Kandaswamy, Umasankar and Adjeroh, Donald A. and Lee, M. C.},
doi = {10.1109/TGRS.2005.852768},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2005 - Kandaswamy - Efficient texture analysis of SAR imagery.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Approximate features,Gabor wavelets,Gray-level cooccurrence matrix (GLCM),Patch reoccurrences,Synthetic aperture radar (SAR) imagery,Texture analysis,texture feature extraction},
mendeley-tags = {texture feature extraction},
number = {9},
pages = {2075--2083},
title = {{Efficient texture analysis of SAR imagery}},
volume = {43},
year = {2005}
}
@techreport{Defra2011,
author = {Keyworth, Steve and Medcalf, Katie and Turton, Nicki and Et. al},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/Making{\_}EO{\_}work{\_}for{\_}UK{\_}biodiv{\_}PART{\_}A{\_}final.pdf:pdf},
institution = {DEFRA, JNCC, Aberystwyth University, Callen-Lenz Consulting},
number = {June},
title = {{Making Earth Observation Work for UK Biodiversity Conservation – Phase 1}},
year = {2011}
}
@inproceedings{Kim2010,
annote = {From Duplicate 2 (Using panchromatic imagery in place of multispectral imagery for kelp detection in water - Kim, Angela M; Olsen, R Chris; Lee, Krista; Jablonski, David)

substituting MSI and NDVI with brightness in panchromatic band},
author = {Kim, Angela M and Olsen, R Chris and Lee, Krista and Jablonski, David},
booktitle = {SPIE},
doi = {10.1117/12.850352},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/Kim, AM - Panchromatic Imagery for Kelp Detection.pdf:pdf},
keywords = {1,be used in cases,for the purposes of,giant kelp,if panchromatic imagery can,in the typical spectral,macrocystis pyrifera,mapping kelp in water,may not be available,motivation and background,panchromatic imagery,range for panchromatic,remote sensing,research is to determine,sun glint removal,the goal of this,where multispectral imagery},
title = {{Using panchromatic imagery in place of multispectral imagery for kelp detection in water}},
volume = {7678 76780},
year = {2010}
}
@article{Ledig2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.04802v5},
author = {Ledig, Christian and Theis, Lucas and Husz, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
eprint = {arXiv:1609.04802v5},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2017 - Ledig, et al - SRGAN - 1609.04802.pdf:pdf},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
year = {2017}
}
@article{Lezoray2008,
abstract = {EURASIP Journal on Advances in Signal Processing},
author = {L{\'{e}}zoray, Olivier and Charrier, Christophe and Cardot, Hubert and Lef{\'{e}}vre, S{\'{e}}bastien},
doi = {10.1155/2008/927950},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2008 - Lezoray O, et al. - Machine Learning in Image Processing.pdf:pdf},
issn = {16876172},
journal = {Eurasip Journal on Advances in Signal Processing},
title = {{Machine learning in image processing}},
volume = {2008},
year = {2008}
}
@article{Li2009,
abstract = {Many remote-sensing satellites can obtain images in multispectral and panchromatic bands. By fusing low-resolution multispectral and high-resolution panchromatic images, one can obtain high-resolution multispectral images. In this paper, an image fusion algorithm based on image restoration is proposed to combine multispectral and panchromatic images. For remote-sensing satellites, the wavelength of the panchromatic band usually covers the wavelengths of the multispectral bands. This relationship between the two kinds of images is useful for fusion. In our approach, the low-resolution multispectral images are first resampled to the scale of the high-resolution panchromatic image. The relationship between these two kinds of images is then used to restore the resampled multispectral images. That is, the resampled multispectral images are modeled as the noisy blurred versions of the ideal multispectral images, and the high-resolution panchromatic image is modeled as a linear combination of the ideal multispectral images plus the observation noise. The ideal high-resolution multispectral images are then estimated based on the panchromatic and the resampled multispectral images. A closed-form solution of the fused images is derived here. Experiments show that the proposed fusion algorithm works effectively in integrating multispectral and panchromatic images.},
annote = {From Duplicate 2 (Fusion of multispectral and panchromatic images using a restoration-based method - Li, Zhenhua; Leung, Henry)

highlights previous approaches
USE THIS GUY'S REFERENCES
DWFT
DWT
a trous
genetic algorithms for optimization
GS
IHS

fuses assuming pan is a linear combination of MS
Discrete Sine Transforms
closed form solution with a genetic algorithm structure for image restoration},
author = {Li, Zhenhua and Leung, Henry},
doi = {10.1109/TGRS.2008.2005639},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2009 - Li Z, Leung H - Fusion of Multispectral and Panchromatic Images Using a Restoration-Based Method.pdf:pdf},
isbn = {0196-2892},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Image fusion,Image restoration,Multispectral image,Panchromatic image,Remote sensing},
number = {5},
pages = {1482--1491},
publisher = {IEEE},
title = {{Fusion of multispectral and panchromatic images using a restoration-based method}},
volume = {47},
year = {2009}
}
@article{Liu2015,
abstract = {Synthetic aperture radar in the application of remote sensing technology is becoming more and more widely because of its all-time and all-weather operation, feature extraction research in high resolution SAR image has become a hot topic of concern. In particular, with the continuous improvement of airborne SAR image resolution, image texture information become more abundant. It's of great significance to classification and extraction. In this paper, a novel method for built-up areas extraction using both statistical and structural features is proposed according to the built-up texture features. First of all, statistical texture features and structural features are respectively extracted by classical method of gray level co-occurrence matrix and method of variogram function, and the direction information is considered in this process. Next, feature weights are calculated innovatively according to the Bhattacharyya distance. Then, all features are weighted fusion. At last, the fused image is classified with K-means classification method and the built-up areas are extracted after post classification process. The proposed method has been tested by domestic airborne P band polarization SAR images, at the same time, two groups of experiments based on the method of statistical texture and the method of structural texture were carried out respectively. On the basis of qualitative analysis, quantitative analysis based on the built-up area selected artificially is enforced, in the relatively simple experimentation area, detection rate is more than 90{\%}, in the relatively complex experimentation area, detection rate is also higher than the other two methods. In the study-area, the results show that this method can effectively and accurately extract built-up areas in high resolution airborne SAR imagery.},
annote = {From Duplicate 2 (Built-up areas extraction in high resolution SAR imagery based on the method of multiple feature weighted fusion - Liu, Xin; Zhang, Ji Xian; Hao, Zheng; Ma, Andong)

discusses various texture

weighted fusion of features
unsupervised (k-means) classigication.},
author = {Liu, Xin and Zhang, Ji Xian and Hao, Zheng and Ma, Andong},
doi = {10.5194/isprsarchives-XL-7-W4-121-2015},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2015 - Liu X, Zhang J, Zhao Z, Ma A - Builtup areas extraction in high resolution SAR imagery based on the method of multiple feature weighted fusion.pdf:pdf},
issn = {16821750},
journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
keywords = {Airborne synthetic aperture radar,Built-up areas,Feature weighted fusion,Gray symbiotic matrix,Variogram,texture feature extraction},
mendeley-tags = {texture feature extraction},
number = {7W4},
pages = {121--125},
title = {{Built-up areas extraction in high resolution SAR imagery based on the method of multiple feature weighted fusion}},
volume = {40},
year = {2015}
}
@article{mdpi2018,
annote = {read pages 72, 153, 172},
author = {Martinsanz, Gonzalo Pajares},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Image Processing in Agriculture and Forestry.pdf:pdf},
isbn = {9783038970972},
journal = {Journal of Imaging},
keywords = {rnal of},
number = {Special Issue},
pages = {224},
title = {{Image Processing in Agriculture and Forestry}},
year = {2018}
}
@article{Masi2016,
author = {Masi, Giuseppe and Cozzolino, Davide and Verdoliva, Luisa and Scarpa, Giuseppe},
doi = {10.3390/rs8070594},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2016 - Masi G, et al. - Pansharpening by CNNs.pdf:pdf},
keywords = {convolutional neural networks,enhancement,machine learning,multiresolution,segmentation,super-resolution},
title = {{Pansharpening by Convolutional Neural Networks}},
year = {2016}
}
@phdthesis{USAF1990,
abstract = {This study evaluates several methods that enhance the spatial resolution of multispectral images using a finer resolution panchromatic image. The resultant hybrid, high resolution, multispectral data set has increased visible interpretation and improved classification accuracy, while preserving the radiometry of the original multispectral images. These methods can therefore be applied to create simulated high resolution multispectral data, as well as to enhance image analysis.},
annote = {From Duplicate 2 (Merging panchromatic and multispectral images for enhanced image analysis - Munechika, C. K.)

tries a number of different things

leading method depends on context;
see page 115 (e.g. classification, maintaining radiometry, visual interpretability

highlights NNs as a potential future pathway},
author = {Munechika, Curtis K.},
booktitle = {United States Air Force (USAF)},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/1990 - Munechika CK - Merging Panchromatic and Multispectral Images for Enhanced Image Analysis.pdf:pdf},
keywords = {*DATA BASES,ACCURACY,CLASSIFICATION,HIGH RESOLUTION,IMAGE PROCESSING,IMAGES,MULTISPECTRAL,RADIOMETRY,RESOLUTION,SPATIAL DISTRIBUTION,USAF},
mendeley-tags = {USAF},
number = {August},
pages = {217},
school = {Rochester Institute of Technology},
title = {{Merging panchromatic and multispectral images for enhanced image analysis}},
type = {Masters Thesis},
url = {http://www.dtic.mil/docs/citations/ADA227902{\%}5Cnhttp://oai.dtic.mil/oai/oai?verb=getRecord{\&}metadataPrefix=html{\&}identifier=ADA227902},
year = {1990}
}
@article{Oetiker2015,
author = {Oetiker, Tobias},
file = {:C$\backslash$:/Program Files/MiKTeX 2.9/doc/latex/acronym/acronym.pdf:pdf},
pages = {1--23},
title = {{A T X 2 ∗ An Acronym Environment for L E $\epsilon$}},
year = {2015}
}
@article{Ouerghemmi2018,
author = {Ouerghemmi, Walid and Gadal, S{\'{e}}bastien and Mozgeris, Gintautas and Ouerghemmi, Walid and Gadal, S{\'{e}}bastien and Mozgeris, Gintautas and Vegetation, Urban},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Ouerghemmi W, et al - [HAL] Urban Vegetation Mapping using Hyperspectral Imagery and Spectral Library.pdf:pdf},
isbn = {9781538671504},
title = {{Urban Vegetation Mapping using Hyperspectral Imagery and Spectral Library To cite this version : HAL Id : hal-01852849}},
year = {2018}
}
@article{Ouerghemmi2018a,
author = {Ouerghemmi, Walid and Mozgeris, Gintautas},
doi = {10.1109/IGARSS.2018.8518893},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Ouerghemmi W, et al - Urban Vegetation Mapping using Hyperspectral Imagery and Spectral Library.pdf:pdf},
isbn = {9781538671504},
number = {July},
title = {{Urban Vegetation Mapping Using Hyperspectral Imagery and Spectral Library}},
year = {2018}
}
@article{Palsson2017,
abstract = {In this paper, we propose a method using a three dimensional convolutional neural network (3-D-CNN) to fuse together multispectral (MS) and hyperspectral (HS) images to obtain a high resolution hyperspectral image. Dimensionality reduction of the hyperspectral image is performed prior to fusion in order to significantly reduce the computational time and make the method more robust to noise. Experiments are performed on a data set simulated using a real hyperspectral image. The results obtained show that the proposed approach is very promising when compared to conventional methods. This is especially true when the hyperspectral image is corrupted by additive noise.},
annote = {From Duplicate 2 (Multispectral and Hyperspectral Image Fusion Using a 3-D-Convolutional Neural Network - Palsson, Frosti; Sveinsson, Johannes R.; Ulfarsson, Magnus O.)

component substitution
multiresolution analysis

read 13 and 14

here he proposes MS+HS fusion

states assumptions clearly - do the same},
archivePrefix = {arXiv},
arxivId = {1706.05249},
author = {Palsson, Frosti and Sveinsson, Johannes R. and Ulfarsson, Magnus O.},
doi = {10.1109/LGRS.2017.2668299},
eprint = {1706.05249},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2017 - Palsson F - MS and HS Image Fusion using a 3D CNN.pdf:pdf},
isbn = {1545-598X},
issn = {1545598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {Convolutional neural networks (CNNs),Deep learning (DL),Hyperspectral (HS),Image fusion,multispectral (MS)},
number = {5},
pages = {639--643},
title = {{Multispectral and Hyperspectral Image Fusion Using a 3-D-Convolutional Neural Network}},
volume = {14},
year = {2017}
}
@unpublished{Palsson2018,
author = {Palsson, Sveinn and Agustsson, Eirikur and Timofte, Radu and {Van Gool}, Luc},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Palsson et al.- cycleGAN for Face Aging - ETH.pdf:pdf},
title = {{Generative Adversarial Style Transfer Networks for Face Aging}},
year = {2018}
}
@book{Pohl2010,
author = {Pohl, C and Genderen, J L Van},
doi = {10.1080/014311698215748},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/1998 - Pohl C, Van Genderen JL - Review article Multisensor image fusion in remote sensing Concepts methods and applications.pdf:pdf},
isbn = {0143116982157},
title = {{Review article Multisensor image fusion in remote sensing : Concepts , methods and applications}},
volume = {1161},
year = {2010}
}
@article{Radford2016,
annote = {DCGAN
unsupervised algorithm},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06434v2},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {arXiv:1511.06434v2},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2016 - Radford A, Metz, L - DCGAN - 1511.06434.pdf:pdf},
pages = {1--16},
title = {{UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL NEURAL NETWORKS}},
year = {2016}
}
@article{Rajabi2013,
abstract = {Hyperspectral imaging, due to providing high spectral resolution images, is one of the most important tools in the remote sensing field. Because of technological restrictions hyperspectral sensors has a limited spatial resolution. On the other hand panchromatic image has a better spatial resolution. Combining this information together can provide a better understanding of the target scene. Spectral unmixing of mixed pixels in hyperspectral images results in spectral signature and abundance fractions of endmembers but gives no information about their location in a mixed pixel. In this paper we have used spectral unmixing results of hyperspectral images and segmentation results of panchromatic image for data fusion. The proposed method has been applied on simulated data using AVRIS Indian Pines datasets. Results show that this method can effectively combine information in hyperspectral and panchromatic images.},
annote = {From Duplicate 2 (Fusion of Hyperspectral and Panchromatic Images Using Spectral Unmixing Results - Rajabi, R.; Ghassemian, H.)

spectral unmixing},
archivePrefix = {arXiv},
arxivId = {1310.5965},
author = {Rajabi, R. and Ghassemian, H.},
doi = {10.5194/isprsarchives-XL-1-W3-333-2013},
eprint = {1310.5965},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2013 - Rajabi R, Ghassemian H - Fusion of Hyperspectral and Panchromatic Images Using Spectral Unmixing Results.pdf:pdf},
issn = {2194-9034},
journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {fusion,hyperspectral imagery,image,linear mixing model,lmm,pan,panchromatic,spectral unmixing},
pages = {333--336},
title = {{Fusion of Hyperspectral and Panchromatic Images Using Spectral Unmixing Results}},
url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-1-W3/333/2013/},
volume = {XL-1/W3},
year = {2013}
}
@phdthesis{Rea2015,
annote = {1st pass reflection 
Used some data that was acquired at significantly different times 
Areas focused: 
Maspalomas Special Natural Reserve 
Teide National Park 
Gives a general outline to: 
Swction 1 Remote Sensing tech 
Section 2 Data Fusion 
Section 3 Image Classification 
Conclusions: 
Seems like some general improvement (7{\%} - 15{\%} depending on situation) 
Strong indicator for the use of data fusion 
Future Work: 
Compare to fresh data 
Test algorithm on other data (non TSX {\&} WorldView-2) 
Effects of Same time acquisition 
Vegetation index fusion 
References include: 
Previous ARTeMISat analysis 
Satellite datasheets for TSX, Sentinel 1, and WV2 
Remote sensing for vegetation/natural resources/and  general 
Image fusion and processing 
A trous wavelet transform 
Texture extraction 
Classification techniques 
Classification workflow 
Maximum Likelihood classifier},
author = {Rea, Raffaele},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2015 - Rea R - MS and SAR Data Fusion based on G-IHS and ATWT for the mapping of natural areas.pdf:pdf},
keywords = {ATWT,Data Fusion,G-IHS,Multispectral,SAR},
mendeley-tags = {ATWT,Data Fusion,G-IHS,Multispectral,SAR},
school = {UNIVERSIT{\`{A}} DEGLI STUDI DI NAPOLI FEDERICO II},
title = {{Multispectral and SAR Data Fusion based on G-IHS and ATWT for the mapping of natural areas}},
type = {Masters Thesis},
year = {2015}
}
@article{Rowe2016,
author = {Rowe, Samantha},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2016 - Rowe, Samantha - Monitoring Mangrove Ecosystems in the Bacalar Region using the Copernicus Constellation.pdf:pdf},
number = {September},
title = {{Monitoring Mangrove Ecosystems in the Bacalar Region using the Copernicus Constellation}},
year = {2016}
}
@article{Roy2018,
abstract = {In this paper we propose a novel texture descriptor called Fractal Weighted Local Binary Pattern (FWLBP). The fractal dimension (FD) measure is relatively invariant to scale-changes, and presents a good correlation with human viewpoint of surface roughness. We have utilized this property to construct a scale-invariant descriptor. Here, the input image is sampled using an augmented form of the local binary pattern (LBP) over three different radii, and then used an indexing operation to assign FD weights to the collected samples. The final histogram of the descriptor has its features calculated using LBP, and its weights computed from the FD image. The proposed descriptor is scale invariant, and is also robust in rotation or reflection, and partially tolerant to noise and illumination changes. In addition, the local fractal dimension is relatively insensitive to the bi-Lipschitz transformations, whereas its extension is adequate to precisely discriminate the fundamental of texture primitives. Experiment results carried out on standard texture databases show that the proposed descriptor achieved better classification rates compared to the state-of-the-art descriptors.},
archivePrefix = {arXiv},
arxivId = {1801.03228},
author = {Roy, Swalpa Kumar and Bhattacharya, Nilavra and Chanda, Bhabatosh and Chaudhuri, Bidyut B. and Ghosh, Dipak Kumar},
eprint = {1801.03228},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Roy SK, et al - FWLBP A Scale Invariant Descriptor for Texture Classification.pdf:pdf},
title = {{FWLBP: A Scale Invariant Descriptor for Texture Classification}},
url = {http://arxiv.org/abs/1801.03228},
year = {2018}
}
@article{SchistadSolberg1997,
abstract = {The discrimination ability of four different methods for texture$\backslash$ncomputation in ERS SAR imagery is examined and compared. Feature$\backslash$nselection methodology and discriminant analysis are applied to find the$\backslash$noptimal combination of texture features. By combining features derived$\backslash$nfrom different texture models, the classification accuracy increased$\backslash$nsignificantly},
annote = {From Duplicate 2 (Texture fusion and feature selection applied to sar imagery - Schistad Solberg, Anne H.; Jain, Anil K.)

SAR image enhancement via fusion for classification accuracy

banks on lit review of texture being a critical component of classification and interpretability of images.

combined methods},
author = {{Schistad Solberg}, Anne H. and Jain, Anil K.},
doi = {10.1109/36.563288},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/1997 - Solberg AHS, Jain AK - Texture Fusion and Feature Selection Applied to SAR Imagery .pdf:pdf},
isbn = {0196-2892 VO - 35},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
number = {2},
pages = {475--479},
title = {{Texture fusion and feature selection applied to sar imagery}},
volume = {35},
year = {1997}
}
@article{Schowengerdt1980,
abstract = {A data compression technique that utilizes a mixture of spatial resolutions for a multispectral scanner is described. Error rates are calculated for two types of scenes, one containing prominent topographic effects, the other of an agricultural area. Improvement in radiometric quality of up to 40 per cent is achieved by application of the reconstruction procedure to the compressed data. - from Author},
author = {Schowengerdt, RA},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/1980 - Schowengerdt RA - Reconstruction of multispatial, multispectral image data using spatial frequency content.pdf:pdf},
issn = {0099-1112},
journal = {Photogrammetric Engineering and Remote Sensing},
keywords = {No. 10,October 1980,PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING,Vol. 46,pp. 1325-1334.},
number = {10},
pages = {1325--1334},
title = {{Reconstruction of multispatial, multispectral image data using spatial frequency content}},
url = {http://adsabs.harvard.edu/abs/1980PgERS..46.1325S},
volume = {46},
year = {1980}
}
@article{Shahdoosti2015,
abstract = {Among the existing fusion algorithms, the wavelet fusion method is the most frequently discussed one in recent publications because the wavelet approach preserves the spectral characteristics of the multispectral image better than other methods. The Brovey is also a popular fusion method used for its ability in preserving the spatial information of the PAN image. This study presents a new fusion approach that integrates the advantages of both the Brovey (which preserves a high degree of spatial information) and the wavelet (which preserves a high degree of spectral information) techniques to reduce the colour distortion of fusion results. Visual and statistical analyzes show that the proposed algorithm clearly improves the merging quality in terms of: correlation coefficient and UIQI; compared to fusion methods including, IHS, Brovey, PCA , HPF, discrete wavelet transform (DWT), and a-trous wavelet. Introduction The satellites observating the earth provide two types of images: several high spectral/low spatial resolution multispectral (MS) images and a low-spectral/high spatial panchromatic (PAN) image. This complementary information are used by image fusion methods (or pan-sharpening methods) to obtain a fused image, having a high spectral/high spatial resolution, which is more informative and interpretative than both initial images. The most commonly used image-fusion methods are those based on the IHS (also known as HSI) [1], [2] and principal component analysis (PCA) [3], [4]. However, these methods can cause spectral distortion in the results [2]. Chavez proposed the HPF(High Pass Filtering) fusion algorithm which has shown better performance in terms of the high-quality synthesis of spectral information [5], [6]. The principle of this method is based on extracting the high-frequency information from the PAN image and injecting it into the MS image. The Box car filter is used for extracting the high-frequency information in this method. However, the ripple in the frequency response of box car filters has some negative impact on the fusion results. The well-known Mallat's fusion algorithm [7] uses an orthonormal basis and it can effectively preserve spectral information, but because of down-sampling operators which are used to implement ordinary discrete wavelet transforms, these transforms are not shift invariant and they can lead to problem in data fusion [8]. To avoid the above-mentioned problem, the discrete wavelet transform known as " {\`{a}} trous " algorithm [9-11] was proposed by eliminating the decimation operators in the wavelet structure. It is a shift-invariant and redundant wavelet transform algorithm based on a multiresolution dyadic scheme. Similar to Mallat's fusion algorithm, {\`{a}} trous can preserve the spectral information of MS images well. The Brovey method is based on the chromaticity transform [13], [14], [15]. It is a simple but powerful method for combining the data acquired by different sensors. The Brovey preserves the spatial information of PAN image well. However distortion of the spectral information is not acceptable in this method. To overcome the deficiency of the Brovey method, a new fusion approach is proposed in this paper. This new method integrates the advantages of both the Brovey (which preserves a high degree of spatial information) and the {\`{a}} trous (which preserves a high degree of spectral information) techniques to preserve both spatial and spectral information of PAN and MS images. To verify the efficiency of the proposed method, visual and statistical assessments are carried out on the MS and PAN data. {\`{A}} trous algorithm This section reviews the {\`{a}} trous fusion method as presented in [11],[15],[16] and briefly discusses the characteristics of these method. Two different strategies can be used in the {\`{a}} trous fusion method which are the additive wavelet (AW) and substitution wavelet (SW). The AW injects the high frequency wavelet planes of the PAN image directly into the MS image. Thus, the radiometric signature of the MS image is changed by this algorithm. This means that the AW method produces redundant geometric information. This method can be formulated as follow:},
author = {Shahdoosti, Hamid Reza},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2015 - Shahdoosti HR - MS {\&} PAN image fusion by combining Brovey and Wavelet methods.pdf:pdf},
journal = {International Conference on Information Technology, Computer {\&} Communication},
keywords = {Brovey Transform,Image fusion,multispectral image,wavelet 2},
title = {{MS and PAN image fusion by combining Brovey and wavelet methods}},
url = {https://arxiv.org/ftp/arxiv/papers/1701/1701.01996.pdf},
year = {2015}
}
@article{Telecomunicazioni2015,
author = {Telecomunicazioni, Ingegneria},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2015 - Rea R - MS and SAR Data Fusion based on G-IHS and ATWT for the mapping of natural areas.pdf:pdf},
title = {{FEDERICO II Tesi di Laurea Multispectral and SAR Data Fusion based on G-IHS}},
year = {2015}
}
@inproceedings{VanEtten2018,
annote = {From Duplicate 2 (You Only Look Twice : Rapid Multi-Scale Object Detection In Satellite Imagery - Van Etten, Adam)

SERIOUS ATTEMPT AT DEEP LEARNING ARCHITECTURES

YOLT pipeline for rapid object detection (e.g. vehicle localization

identifies issues with deep learning techniques for satellite imagery:
small spatial extent
rotational invariance
lack of training data
CAN'T JUST SLAP ON EXISTING NETWORKS - ultra high resolutions. not 256x256 like alexnet and googlenet seem to like

data augmentation is critical to rotational invariance and vatiance in weather and sensors and other conditions

consider that sale invariance may suffer due to range of variety - e.g. small alleys and large highways are both still roads},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.09512v1},
author = {{Van Etten}, Adam},
booktitle = {Computer Vision},
eprint = {arXiv:1805.09512v1},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - VanEtten A - YOLT Rapid Multi-scale Object Detection in Satellite Imagery.pdf:pdf},
keywords = {YOLT,computer vision,object detection,satellite imagery},
mendeley-tags = {YOLT},
organization = {CosmiQ Works},
title = {{You Only Look Twice : Rapid Multi-Scale Object Detection In Satellite Imagery}},
year = {2018}
}
@misc{VanEtten,
annote = {From Duplicate 2 (Pan to MS Object Detection Performance as a Function of Imaging Bands - Van Etten, Adam; Cohn, Lee)

SpaceNet Dataset - labelled dataset already pansharpened (30cm GSD)},
author = {{Van Etten}, Adam and Cohn, Lee},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/VanEtten A, Cohn L - Pan to MS Object Detection Performance as a Function of Imaging Bands.pdf:pdf},
title = {{Pan to MS Object Detection Performance as a Function of Imaging Bands}},
url = {https://medium/com/the-downlinq/panchromatic-to-multispectral-object-detection-performance-as-a-function-of-imaging-bands-51ecaaa3d56},
urldate = {2018},
year = {2017}
}
@article{Vivone2014,
abstract = {The pansharpening process has the purpose of building a high-resolution multispectral image by fusing low spatial resolution multispectral and high-resolution panchromatic observations. A very credited method to pursue this goal relies upon the injection of details extracted from the panchromatic image into an upsampled version of the low-resolution multispectral image. In this letter, we compare two different injection methodologies and motivate the superiority of contrast-based methods both by physical consideration and by numerical tests carried out on remotely sensed data acquired by IKONOS and Quickbird sensors.},
author = {Vivone, Gemine and Restaino, Rocco and Mura, Mauro Dalla and Licciardi, Giorgio and Chanussot, Jocelyn},
doi = {10.1109/LGRS.2013.2281996},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2014 - Vivone G, et al. -Contrast and Error-Based FUsion Schemes for Multispectral Image Pansharpening.pdf:pdf},
isbn = {1545-598X},
issn = {1545598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {High pass modulation,Injection models,Modulation transfer functions,Pansharpening},
number = {5},
pages = {930--934},
publisher = {IEEE},
title = {{Contrast and error-based fusion schemes for multispectral image pansharpening}},
volume = {11},
year = {2014}
}
@article{Wang2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.00219v2},
author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Loy, Chen Change and Qiao, Yu and Tang, Xiaoou},
eprint = {arXiv:1809.00219v2},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Wang X, et al. - ESRGAN - 1809.00219.pdf:pdf},
title = {{ESRGAN : Enhanced Super-Resolution}},
year = {2018}
}
@unpublished{Yahia2018,
author = {Yahia, Oualid and Guida, Raffaella and Iervolino, Pasquale},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Yahia Oualid - Feature Level Data Fusion for Soil Moisture Content Estimation.pdf:pdf},
keywords = {feature level fusion},
mendeley-tags = {feature level fusion},
pages = {0--5},
title = {{Sentinel - 1 and Landsat - 8 feature level fusion for soil moisture content estimation}},
year = {2018}
}
@article{Yahia2018a,
author = {Yahia, Oualid and Guida, Raffaella and Iervolino, Pasquale},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2018 - Yahia Oualid - IGARSS Decision Level Data Fusion for Soil Moisture Content Estimation.pdf:pdf},
pages = {1--4},
title = {{WEIGHTS BASED DECISION LEVEL DATA FUSION OF LANDSAT-8 AND SENTINEL-1}},
year = {2018}
}
@article{Yamanaka2017,
abstract = {We propose a highly efficient and faster Single Image Super-Resolution (SISR) model with Deep Convolutional neural networks (Deep CNN). Deep CNN have recently shown that they have a significant reconstruction performance on single-image super-resolution. Current trend is using deeper CNN layers to improve performance. However, deep models demand larger computation resources and is not suitable for network edge devices like mobile, tablet and IoT devices. Our model achieves state of the art reconstruction performance with at least 10 times lower calculation cost by Deep CNN with Residual Net, Skip Connection and Network in Network (DCSCN). A combination of Deep CNNs and Skip connection layers is used as a feature extractor for image features on both local and global area. Parallelized 1x1 CNNs, like the one called Network in Network, is also used for image reconstruction. That structure reduces the dimensions of the previous layer's output for faster computation with less information loss, and make it possible to process original images directly. Also we optimize the number of layers and filters of each CNN to significantly reduce the calculation cost. Thus, the proposed algorithm not only achieves the state of the art performance but also achieves faster and efficient computation. Code is available at https://github.com/jiny2001/dcscn-super-resolution.},
author = {Yamanaka, Jin and Kuwashima, Shigesumi and Kurita, Takio},
doi = {10.1007/978-3-319-70096-0_23},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2017 - Yamanaka J, et al - Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network - 1707.05425.pdf:pdf},
isbn = {9783319700953},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Deep CNN,Deep learning,Image super resolution,Network in network,Residual net,Skip connection},
pages = {217--225},
title = {{Fast and Accurate Image Super Resolution by Deep CNN with Skip Connection and Network in Network}},
volume = {10635 LNCS},
year = {2017}
}
@misc{Zhang,
annote = {From Duplicate 2 (Panchromatic ( Pan ) and Multispectral ( MS ) Image Fusion or Image Merging Using UNB-Pansharp - Zhang, Yun)

novel method for pansharpening},
author = {Zhang, Yun},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/Zhang, Y - Pan and MS Image FUsion or Image Merging using UNB-Pansharp.pdf:pdf},
institution = {University of New Brunswick},
keywords = {Zhang,panchromatic,pansharpening,unb},
mendeley-tags = {panchromatic,pansharpening,unb},
publisher = {UNB},
title = {{Panchromatic ( Pan ) and Multispectral ( MS ) Image Fusion or Image Merging Using UNB-Pansharp}}
}
@book{Zheng,
author = {Zheng, Yufeng},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/Yufeng Zheng (ed) - Image Fusion and Its Applications.pdf:pdf},
isbn = {9789533071824},
title = {{IMAGE FUSION Edited by Yufeng Zheng}}
}
@article{Zhou2012,
author = {Zhou, Xizhen and Xia, Lihua and Pan, Zhuokun and Xu, Shan},
doi = {10.1109/RSETE.2012.6260361},
file = {:E$\backslash$:/OneDrive - University of Surrey/Dissert/references/2012 - Zhou et al - A New Method for Urban Vegetation Remotely Sensed Monitoring.pdf:pdf},
isbn = {9781467308755},
journal = {2012 2nd International Conference on Remote Sensing, Environment and Transportation Engineering},
keywords = {-urban vegetation,benefits of vegetation mitigation,ecology service functions,feature,in the urban heat,investigating,island,methods,spectral,this has,to explore new research,urban heat island},
pages = {1--3},
publisher = {IEEE},
title = {{A New Method for Urban Vegetation Remotely Sensed Monitoring}},
year = {2012}
}
